# Objective News - The Process
By: Xild076 (Harry Yin)\
harry.d.yin.gpc@gmail.com

### Table of Contents
1. [Abstract](#1-abstract)
2. [Introduction](#2-introduction)
3. [Methodology](#3-methodology)
    1. [Grouping](#31-grouping)
    2. [Objectifying](#32-objectifying)
    3. [Summarizing](#33-summarizing)
    4. [Reliability](#34-reliability)
4. [Implementation](#4-implementation)
    1. [Grouping](#41-grouping)
    2. [Objectifying](#42-objectifying)
    3. [Summarizing](#43-summarizing)
    4. [Reliability](#44-reliability)
5. [Results](#5-results)
    1. [Grouping](#51-grouping)
    2. [Objectifying](#52-objectifying)
    3. [Summarizing](#53-summarizing)
    4. [Overall (Article Analysis)](#54-overall-article-analysis)
6. [Discussion](#6-discussion)
7. [Future Plans](#7-future-plans)
8. [Conclusion](#8-conclusion)
9. [Sources](#9-sources)

## 1. Abstract

## 2. Introduction

## 3. Methodology

### 3.1. Grouping

### 3.2. Objectifying

### 3.3. Summarizing

### 3.4. Reliability

- Most reliable source sets baseline, the lower the more the score leans low
- Less weight to unreliable sources, the higher the score the less it contributes
- Reality check, if too many unreliable sources, it adds a penalty
- Balance between reliable and unreliable, strong reliable source pulls the final score towards reliability, multiple unreliable sources can still drag down the reliability if they outweigh the good ones
- It balances optimism from the star player (reliable sources) with realism about the teamâ€™s overall strength (unreliable sources)

## 4. Implementation

### 4.1. Grouping

### 4.2. Objectifying

### 4.3. Summarizing

### 4.4. Reliability

## 5. Results

### 5.1. Grouping

### 5.2. Objectifying

### 5.3. Summarizing

### 5.4. Overall (Article Analysis)

## 6. Discussion

## 7. Future Plans

## 8. Conclusion

## 6. Discussion

## 7. Future Plans

## 8. Conclusion

## 9. Sources
https://arxiv.org/pdf/1706.03762
https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html
https://medium.com/@wangdk93/multihead-attention-from-scratch-6fd6f99b9651
https://www.slingacademy.com/article/understanding-multi-head-attention-for-nlp-models-in-pytorch/
https://armanasq.github.io/nlp/self-attention/
https://arxiv.org/pdf/2211.01071
https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html
https://medium.com/@srinidhikarjol/how-positional-embeddings-work-in-self-attention-ef74e99b6316
https://www.geeksforgeeks.org/logging-in-python/
https://arxiv.org/pdf/1706.03762